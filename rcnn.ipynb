{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import *\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "from torchvision.ops.feature_pyramid_network import FeaturePyramidNetwork\n",
    "from torchvision.models.detection.backbone_utils import BackboneWithFPN\n",
    "from torchvision.ops.feature_pyramid_network import ExtraFPNBlock\n",
    "from torchvision.models import resnet50\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "import torchvision\n",
    "\n",
    "# load image\n",
    "dataset = CXRDataset(\n",
    "        root='/home/ec2-user/data/MIMIC-1105', \n",
    "        image_dir='downsized',\n",
    "        ann_file='annotations_downsized.json',\n",
    "        transforms=get_transform(train=False),\n",
    "        )\n",
    "\n",
    "image = dataset[0][0].unsqueeze(0)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "image = image.to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Faster rcnn with default backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/rcnn/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/rcnn/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'boxes': tensor([[  58.1865,    0.0000,  998.8453, 1004.9954],\n",
      "        [   0.0000,    0.0000,  483.1834,  996.5087],\n",
      "        [ 536.1624,   10.0707, 1024.0000,  884.0034],\n",
      "        [ 148.8860,  283.5781,  194.8409,  326.7523],\n",
      "        [ 725.3314,   92.8112,  768.7886,  123.4762],\n",
      "        [ 140.9995,  346.8251,  908.9592,  985.2390],\n",
      "        [ 132.5804,  304.3146,  919.8915, 1015.0481],\n",
      "        [  90.7145,    0.0000,  149.0744,   69.1741]], device='cuda:0'), 'labels': tensor([ 1,  1,  1, 44, 37, 75,  3, 10], device='cuda:0'), 'scores': tensor([0.4006, 0.2551, 0.1435, 0.1241, 0.0708, 0.0658, 0.0555, 0.0512],\n",
      "       device='cuda:0')}]\n"
     ]
    }
   ],
   "source": [
    "# Load the pre-trained Faster R-CNN model\n",
    "model = fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "# Perform object detection\n",
    "with torch.no_grad():\n",
    "    predictions = model(image)\n",
    "\n",
    "# Print the predictions (bounding boxes and class scores)\n",
    "print(predictions)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Faster rcnn with different backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/rcnn/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() got multiple values for argument 'backbone'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 52\u001b[0m\n\u001b[1;32m     45\u001b[0m model \u001b[39m=\u001b[39m FasterRCNN(\n\u001b[1;32m     46\u001b[0m     backbone,\n\u001b[1;32m     47\u001b[0m     num_classes\u001b[39m=\u001b[39m\u001b[39m91\u001b[39m,  \u001b[39m# Change this to the number of classes in your dataset\u001b[39;00m\n\u001b[1;32m     48\u001b[0m     rpn_anchor_generator\u001b[39m=\u001b[39manchor_generator,\n\u001b[1;32m     49\u001b[0m )\n\u001b[1;32m     51\u001b[0m \u001b[39m# Load the pre-trained Faster R-CNN model with the custom backbone\u001b[39;00m\n\u001b[0;32m---> 52\u001b[0m model \u001b[39m=\u001b[39m fasterrcnn_resnet50_fpn(\n\u001b[1;32m     53\u001b[0m     backbone\u001b[39m=\u001b[39;49mbackbone,\n\u001b[1;32m     54\u001b[0m     pretrained\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     55\u001b[0m     anchor_generator\u001b[39m=\u001b[39;49manchor_generator,\n\u001b[1;32m     56\u001b[0m     num_classes\u001b[39m=\u001b[39;49m\u001b[39m91\u001b[39;49m,  \u001b[39m# Change this to the number of classes in your dataset\u001b[39;49;00m\n\u001b[1;32m     57\u001b[0m )\n\u001b[1;32m     59\u001b[0m model\u001b[39m.\u001b[39meval()\n\u001b[1;32m     60\u001b[0m model\u001b[39m.\u001b[39mto(device)\n",
      "File \u001b[0;32m/opt/conda/envs/rcnn/lib/python3.9/site-packages/torchvision/models/_utils.py:142\u001b[0m, in \u001b[0;36mkwonly_to_pos_or_kw.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    135\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    136\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUsing \u001b[39m\u001b[39m{\u001b[39;00msequence_to_str(\u001b[39mtuple\u001b[39m(keyword_only_kwargs\u001b[39m.\u001b[39mkeys()),\u001b[39m \u001b[39mseparate_last\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mand \u001b[39m\u001b[39m'\u001b[39m)\u001b[39m}\u001b[39;00m\u001b[39m as positional \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    137\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter(s) is deprecated since 0.13 and may be removed in the future. Please use keyword parameter(s) \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    138\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39minstead.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    139\u001b[0m     )\n\u001b[1;32m    140\u001b[0m     kwargs\u001b[39m.\u001b[39mupdate(keyword_only_kwargs)\n\u001b[0;32m--> 142\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/conda/envs/rcnn/lib/python3.9/site-packages/torchvision/models/_utils.py:228\u001b[0m, in \u001b[0;36mhandle_legacy_interface.<locals>.outer_wrapper.<locals>.inner_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    225\u001b[0m     \u001b[39mdel\u001b[39;00m kwargs[pretrained_param]\n\u001b[1;32m    226\u001b[0m     kwargs[weights_param] \u001b[39m=\u001b[39m default_weights_arg\n\u001b[0;32m--> 228\u001b[0m \u001b[39mreturn\u001b[39;00m builder(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/conda/envs/rcnn/lib/python3.9/site-packages/torchvision/models/detection/faster_rcnn.py:563\u001b[0m, in \u001b[0;36mfasterrcnn_resnet50_fpn\u001b[0;34m(weights, progress, num_classes, weights_backbone, trainable_backbone_layers, **kwargs)\u001b[0m\n\u001b[1;32m    561\u001b[0m backbone \u001b[39m=\u001b[39m resnet50(weights\u001b[39m=\u001b[39mweights_backbone, progress\u001b[39m=\u001b[39mprogress, norm_layer\u001b[39m=\u001b[39mnorm_layer)\n\u001b[1;32m    562\u001b[0m backbone \u001b[39m=\u001b[39m _resnet_fpn_extractor(backbone, trainable_backbone_layers)\n\u001b[0;32m--> 563\u001b[0m model \u001b[39m=\u001b[39m FasterRCNN(backbone, num_classes\u001b[39m=\u001b[39;49mnum_classes, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    565\u001b[0m \u001b[39mif\u001b[39;00m weights \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    566\u001b[0m     model\u001b[39m.\u001b[39mload_state_dict(weights\u001b[39m.\u001b[39mget_state_dict(progress\u001b[39m=\u001b[39mprogress))\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got multiple values for argument 'backbone'"
     ]
    }
   ],
   "source": [
    "class LastLevelMaxPool(ExtraFPNBlock):\n",
    "    \"\"\"\n",
    "    Applies a max pooling operation over the non-spatial dimensions of the input.\n",
    "    \"\"\"\n",
    "    def forward(self, x, names):\n",
    "        result = {}\n",
    "        for name, feature in zip(names, x):\n",
    "            result[name] = feature.max(dim=-1)[0].max(dim=-1)[0]\n",
    "        return result\n",
    "\n",
    "    def output_shape(self, in_channels_list):\n",
    "        return {name: chan for name, chan in in_channels_list.items()}\n",
    "\n",
    "\n",
    "# Load the pre-trained ResNet-50 model\n",
    "backbone = resnet50(pretrained=True)\n",
    "\n",
    "# Set the number of output channels (2048 for ResNet-50)\n",
    "backbone.out_channels = 2048\n",
    "\n",
    "# Define the return layers\n",
    "return_layers = {'layer4': '0'}\n",
    "\n",
    "# Create the FPN (Feature Pyramid Network) using the custom backbone\n",
    "fpn = FeaturePyramidNetwork(\n",
    "    in_channels_list=[backbone.out_channels],\n",
    "    out_channels=backbone.out_channels,\n",
    "    extra_blocks=LastLevelMaxPool(),\n",
    ")\n",
    "\n",
    "# Combine the custom backbone and FPN\n",
    "backbone = BackboneWithFPN(\n",
    "    backbone,\n",
    "    return_layers=return_layers,\n",
    "    in_channels_list=[backbone.out_channels],\n",
    "    out_channels=backbone.out_channels,\n",
    ")\n",
    "\n",
    "# Create the anchor generator\n",
    "anchor_sizes = ((32,), (64,), (128,), (256,), (512,))\n",
    "aspect_ratios = ((0.5, 1.0, 2.0),) * len(anchor_sizes)\n",
    "anchor_generator = AnchorGenerator(sizes=anchor_sizes, aspect_ratios=aspect_ratios)\n",
    "\n",
    "# Create the Faster R-CNN model with the custom backbone\n",
    "model = FasterRCNN(\n",
    "    backbone,\n",
    "    num_classes=91,  # Change this to the number of classes in your dataset\n",
    "    rpn_anchor_generator=anchor_generator,\n",
    ")\n",
    "\n",
    "# Load the pre-trained Faster R-CNN model with the custom backbone\n",
    "model = fasterrcnn_resnet50_fpn(\n",
    "    backbone=backbone,\n",
    "    pretrained=False,\n",
    "    anchor_generator=anchor_generator,\n",
    "    num_classes=91,  # Change this to the number of classes in your dataset\n",
    ")\n",
    "\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "# Perform object detection\n",
    "with torch.no_grad():\n",
    "    predictions = model(image)\n",
    "\n",
    "# Print the predictions (bounding boxes and class scores)\n",
    "print(predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rcnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
